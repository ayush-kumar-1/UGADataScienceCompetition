\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{letterpaper, total={7in, 10in} }
\usepackage{setspace}
\setstretch{1.2}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{appendix}

\title{UGA Data Science Competition 2021}
\author{Ayush Kumar, Chloe Phelps, Faisal Hossain, Nicholas Sung}
\date{April 15, 2021}

\begin{document} 
	
	\maketitle
	\tableofcontents
	\newpage
	
	\section{Exploratory Data Analysis and Preprocessing}
	
	\subsection{Summary Statistics for Continuous Variables}
	
	We begin our exploratory data analysis by looking into the data, and determining the type of the data for each column. For continuous numerical data, we want to visualize the distribution using histograms as well as taking a look at the following summary statistics: mean, standard deviation, the minimum and the maximum. For the categorical data we want to take a look at the possible categories, and the frequency of each category. 
	
	Using pandas and the information sheet here are the numerical variables: 
	

	\input{numerical.tex}
	
	By looking at the descriptive statistics we can see if there are any major outliers, and determine how we can use these variables in our data. We see that minimum and maximum values for many of the continuous variable are very extreme for Total Credit Debt, Average Credit Debt, and Credit past due amount. We also see that many variables seem to have very similar distributions, which may be a problem in regards to multi-collinearity. To investigate this we can create a correlation matrix and re-scale certain variables to try to preserve information, while combating multi-collinearity. 
	
	\begin{center}
			\includegraphics[scale = 0.15]{../notebooks/dist.png}
	\end{center}


	Looking at the histogram gives us a better picture of the distributions of the variables because it allows us to visualize the shapes of the variables. We can see that the outliers for Average Credit Debt are seriously impacting the distribution in comparison to Total Credit Debt. This visualization also allows us to look at some variables which by their description seem to be numerical, but display behavior that is more characteristic of categorical variables. Variables like the number of mortgages past due or non-mortgages past due would be better suited to being dummy variables rather than continuous ones. 
	
	\begin{center}
		\includegraphics[scale = 0.3]{../notebooks/CorMat.png}
	\end{center}
	
	Taking a look at the correlation matrix we can see multiple points of high collinearity between variables that are similar in nature due to the time that they are recorded or other inherent similarities. Based on this correlation matrix we can see that some of the variables may need to be rescaled to preserve as much information as possible, but bring down the high collinearity. Another approach would be to carry on with all the variables and not put too much stock into the individual variable t-tests and instead focus on joint F-tests and ensure that the variables are being properly captured. 
	
	\subsection{Summary Statistics for Categorical Variables}
	
	\begin{center}
		\includegraphics[scale=0.15]{../notebooks/counts.png}
	\end{center}
	
	We can take a look at the counts and the relative frequencies for the variables and make some observations. For many of these variables it would be wise 
	to make multiple dummy variables to allow for better predictive models. States and card\_open\_36\_month\_num should be split into multiple variables for the models. The other variables are either in a form that already mimics a dummy variable, or they can easily be converted into 0 or 1 dummy variables because given the small frequency numbers higher than one appear it would be superfluous to have multiple dummy variables for the different variables. Further analysis is required for the State dummy variables because there need to be substantial differences between the different states to add them as dummy variables. 
	
	The analysis we choose for adding state dummy variables is to simply take a look a random sample of observations and plot a subset of two variables that are representative of the data available and plot them by state. Based on the graphs on page 5 we can see that there are some patterns however weak in the data and we should take them into account when building the model. The significance of the state dummy variables can be assessed at a later time after the models have been built. 
	
	\begin{center}
		\includegraphics[scale = 0.15]{../notebooks/states.png}	
	\end{center}
	
	\subsection{Model Selection}
	
	In addition to the Logistic Regression we were asked to choose between a Feed-Forward Neural Network, a Random Forest Classifier, or XGBoost. Due to the high number of possible interaction effects as well as the high number of continuous variables our team has opted to use the Feed-Forward Neural Network. These networks can be useful because they allow for a very high number of interaction effects without extensive testing. We will use a sigmoid function as our last response function to ensure the model output is similar to the output of the logistic regression.
	
	Another challenge in model building will be the relatively low number of observations that do default on their credit debt. This makes it extraordinarily difficult to make sure our models are genuine improvements over the baseline model of predicting 0 for every observations. This baseline model has an accuracy of 92\% so it will be important that we do not over-depend on accuracy as a measure of model fit. Precision, recall, and f1-scores will be much better indicator of model fit, and should be used when tuning hyper-parameters. 
	
	\subsection{Preprocessing Transforms}
	
	We will be using data in numpy arrays, but first we must overcome some of the challenges of this approach. We must have numerical data for every observation, and there can be no missing values or the logistic regression model will not function properly. Our \verb|load_data(path: String): X, y| function will conduct the following transforms on the data to prepare it for model use. 
	
	\begin{enumerate}
		\item Import data from a csv into a pandas dataframe
		\item Impute missing numeric values using the expected value of a column
		\item Turn the following rows into dummy variables based on various levels: States, non\_mtg\_*, card\_open\_*, auto\_open\_*
		\item Split the dataset into X - the data matrix and y - the response column
	\end{enumerate}
	
	\section{Logistic Regression Model}
	
	\subsection{Baseline Model}
	
	Using the preprocessed data as discussed above we trained a logistic regression model from sklearn without tuning any hyper parameters. 
	The following were the model coefficients and performance statistics on the testing dataset. 
	

	\begin{tabular}{c|c|c|c|c|c}
	\hline
			& 		& 	precision &  recall  & f1-score   & support \\ \hline
			
			&  0.0	  &		0.93 &     0.99   &  0.96    &  4599 \\ \hline
			&  1.0    &		0.72 &     0.20   &   0.31   &    401 \\ \hline
			
			accuracy & & & &                       					0.93     & 5000 \\ \hline
			macro avg  & &   				0.83   &   0.60  &    0.64  &    5000  \\ \hline
			weighted avg  & &   			0.92   &   0.93  &    0.91   &   5000 \\ \hline
	\end{tabular}					

	The coefficients and interpretation of them is available in Appendix A. The scores can help us assess model performance. The initial news seems to be good with an overall accuracy of 93\%, but this is only marginal better than baseline model. The precision of the model is also ok with a score of 0.92. This means that of all the observations identified by the model as clients who will default we are correct around 72\% of the time. The worrisome part of this model is the recall. A recall of 0.20 implies that we are missing nearly 4/5 of all clients who will end up defaulting. In other words this model is extremely good at identifying that a client will NOT default, but in the process misses a lot of clients who will default. This is all assuming a decision boundary of 0.5, which we will be optimizing. If the firm wants to minimize default risk it will have to increase the decision boundary, and if a firm wants to maximize the amount of credit they are lending the decision boundary should be decreased. Our team believes that defaults are much more costly, and we should try to tune the decision boundary in  way that is beneficial to the end goal of reducing defaults. 
	
	It should also be noted that for a logistic regression we can directly interpret the variables and the effect they will have on the overall model. This will be discussed in greater detail in the explanation section of the report.

	\subsection{Tuning Decision Boundary}
	
	We will be using an algorithm that tries to maximize recall while ensuring that precision does not fall below a certain level. We will be able to test multiple values of the level we want to let the precision fall down to. We will be using a variable learning rate that decreases in a reciprocal fashion every iteration. We will be updating the decision boundary by subtracting the previous boundary times the learning rate for that iteration. 
	
	$$ \eta_{t+1} = \eta_0/t $$ 
	
	$$ threshold_{t+1} = threshold_t*eta_t $$
	
	This is to ensure that we arrive at a decision boundary that converges. The method signature is as follows: \verb|tune_threshold(y, yhat, eta, plev): Double|. The plev is the level of precision we are willing to go down to and by generating many thresholds at different levels of precision we will be able to generate graphs of where the ideal decision boundary may lie. We assume an initial threshold of 0.5 and we will tune from there. Using this algorithm our group tested every precision between 0 and 1 with an increment of 0.1 and plotted the results. 
	
	\begin{center}
		\includegraphics[scale = 0.4]{../notebooks/precisionvsthreshold.png}
	\end{center}
	
	Based on the results it seems that precision is not very sensitive for most values between 0.6 and 1.0 and between 0.0 and 0.6. This behavior is indicative of the precision recall trade-off in our model. To have an substantial gains of recall we would have to expand the number of applications the model will reject be quite a heavy amount. We can see that in the following precision and recall curve for our logistic regression model. 
	
	\begin{center}
		\includegraphics[scale=0.4]{../notebooks/pr_curve_lr.png}
	\end{center}

	We see much of the same behavior we expected from our initial analysis of the precision-threshold curve. Around a precision of 0.5 we begin to see a substantial gain of recall at the cost of precision. Based on this our team recommends using a decision boundary based on preserving a precision of 0.5 and attempting to get much recall as we can. This will reduce our overall accuracy, but ensure that we catch more people who are likely to default on their credit loans minimizing the risk to the firm. This decision boundary for precision level of 0.5 happens to be 0.1514. Here is a summary of model fit after using this decision boundary on the test dataset. The decision boundary was tuned using the validation dataset. 
	
	
	\begin{tabular}{c|c|c|c|c|c}
	\hline
				& 		& 	precision &  recall  & f1-score   & support \\ \hline
	
				&  0.0	&		0.95 &     0.98   &  0.96   &  4599 \\ \hline
				&  1.0  &		0.61 &     0.37   &  0.46   &   401 \\ \hline
	
	accuracy    & & & &                       		  0.93   &   5000 \\ \hline
	macro avg   & &   			0.78   &   0.67  &    0.71   &   5000  \\ \hline
	weighted avg  & &   		0.92   &   0.93  &    0.92   &   5000 \\ \hline
	\end{tabular}	
	 
	
	When using the testing dataset we have greatly improved the recall of our model, while sacrificing a small amount of precision. This means the firm can still be extremely confident in knowing the model will rarely classify a non-defaulter as someone who will default, and will be assuming less risk in terms of trying to catch as many defaulters as possible. 
	
	
	\section{Feed Forward Neural Network Model}
	
	\subsection{Initial Model}
	
	The feed-forward neural network was chosen because of its ability to capture nonlinear interaction effects that may exist between variables. Some downsides of this type of network include its black-box nature, the cost of training, and the possibility of arriving at a local optimum rather than a global optimum. We must take extreme care to ensure that we do not accidentally converge at an incorrect minimum. Our model will be using a single hidden layer with a relu activation function and a single output node of with a sigmoid activation function. We will start with a model with a number of hidden nodes equal to the number of input features and a loss based on binary cross entropy. 
	
	\begin{tabular}{c|c|c|c|c|c}
	\hline
	& 		& 	precision &  recall  & f1-score   & support \\ \hline
	
	&  0.0	&		0.93 &     0.99   &  0.96   &  4599 \\ \hline
	&  1.0  &		0.76 &     0.19   &  0.31   &   401 \\ \hline
	
	accuracy    & & & &                       		  0.93   &   5000 \\ \hline
	macro avg   & &   			0.85   &   0.59  &    0.64   &   5000  \\ \hline
	weighted avg  & &   		0.92   &   0.93  &    0.91   &   5000 \\ \hline
	\end{tabular}
	
	Our initial model has a similar problem to the original logistic regression, but before be begin tuning the decision curve, we can make adjustments to the model architecture. 
	
	\subsection{Tuning Model Width}
	
	
	The first item to consider is the complexity of the model. We may be over or under parameterize, and this can be tuned by simply changing the number of elements in the hidden node. Here are graphs of precision, recall, and accuracy, as we change the number of nodes in the hidden layer. 
	
	
	\includegraphics[scale=0.4]{../notebooks/nn_width.png}
	
	The testing of model with different widths shows that the model is unstable, and that there are no real benefits to increasing model complexity. The most stable models were between 25-30 and so we will be sticking with 32 as the width for our model. The models were only trained for 25 epochs. We will be increasing the training time to ensure that our algorithms are generating stable models. 
	
	\subsection{Model Stability}
	
	To ensure model stability we are training our feed forward neural network 100 times over 100 epochs and then checking the distribution of precision and recall scores to ensure that our model is stable. 
	
	

	
	\subsection{Comparison with Logistic Regression}
	
	\section{Future Decision Making}
	
	\subsection{Previous Customers \& Bias}
	
	\subsection{Explaining Model Decisions}
	
	
	\appendix 
	
	\section{Logistic Regression Interpretation of Coefficients}
	
	\input{coef.tex}
	
\end{document}	